{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### News Headlines Classification\n",
        "\n",
        "\n",
        "This notebook demonstrates a complete workflow for training a Long Short-Term Memory (LSTM) neural network to **classify news headlines into four categories: entertainment, business, science/tech, and health.**\n",
        "\n",
        "**Key Steps:**\n",
        "\n",
        "1.  **Imports**: Essential libraries for data handling (pandas, numpy), deep learning (PyTorch), and natural language processing (NLTK, scikit-learn) are imported.\n",
        "\n",
        "2.  **Data Loading**: News headlines and their corresponding categories are loaded from a CSV file. The dataset is shuffled, and categorical labels are encoded into numerical representations (0-3).\n",
        "\n",
        "3.  **Text Preprocessing**: A custom function tokenizes headlines, converts them to lowercase, removes non-alphabetic characters, and filters out common English stopwords. The processed tokens are stored.\n",
        "\n",
        "4.  **Vocabulary Building**: A vocabulary is constructed from all unique tokens in the dataset. A `word2idx` mapping is created, assigning a unique integer ID to each of the most frequent 20,000 words.\n",
        "\n",
        "5.  **Tokens to Sequences**: Tokenized headlines are converted into numerical sequences using the `word2idx` mapping. Sequences are padded or truncated to a fixed length of 30, preparing them for input to the LSTM model.\n",
        "\n",
        "6.  **Train-Test Split**: The numerical sequences and their labels are split into training and testing sets, with 75% for training and 25% for testing, ensuring stratified sampling.\n",
        "\n",
        "7.  **PyTorch Dataset & DataLoader**: A custom `NewsDataset` class is defined to handle the training and testing data, and `DataLoader` objects are created to efficiently batch and load data during training and evaluation.\n",
        "\n",
        "8.  **LSTM Model Definition**: A bidirectional LSTM-based classifier (`LSTMClassifier`) is defined. It includes an embedding layer, a bidirectional LSTM layer, a dropout layer, and a final linear layer for classification. The model, loss function (CrossEntropyLoss with class weights), and optimizer (Adam) are initialized.\n",
        "\n",
        "9.  **Training**: The model is trained for 5 epochs. For each epoch, it iterates through the training data, performs forward and backward passes, and updates model weights using the Adam optimizer, printing the loss at the end of each epoch.\n",
        "\n",
        "10. **Evaluation**: After training, the model's performance is evaluated on the test set. Accuracy and a detailed classification report (precision, recall, f1-score) are printed to assess the model's performance across different news categories.\n",
        "\n",
        "11. **Prediction Example**: A demonstration of how to preprocess a new unseen headline and use the trained LSTM model to predict its category."
      ],
      "metadata": {
        "id": "R5eZgBPX4bfu"
      },
      "id": "R5eZgBPX4bfu"
    },
    {
      "cell_type": "markdown",
      "id": "ff8f09f0",
      "metadata": {
        "id": "ff8f09f0"
      },
      "source": [
        "<p style=\"\n",
        "  background: #ffffff;\n",
        "  border: 5px solid #9f0b0bff;\n",
        "  border-radius: 18px;\n",
        "  padding: 30px 180px;\n",
        "  text-align: center;\n",
        "  text-shadow: 1px 1px 3px rgba(0, 0, 0, 0.4);\n",
        "  font-family: 'serif';\n",
        "  color: #9f0b0bff;\n",
        "  font-size: 3.8rem;\n",
        "  width: fit-content;\n",
        "  margin: 20px auto;\n",
        "\">\n",
        "  Imports <br>\n",
        "  <span style=\"\n",
        "  text-align: center;\n",
        "  text-shadow: 1px 1px 3px rgba(0, 0, 0, 0.4);\n",
        "  font-family: 'serif';\n",
        "  color: #9f0b0bff;\n",
        "  font-size: 1.6rem;\n",
        "  width: fit-content;\n",
        "\">BY: Genia</span>\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b19d597b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b19d597b",
        "outputId": "0db4c748-35bc-4a55-a885-c2e8a2262160"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from collections import Counter\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79c4d2cd",
      "metadata": {
        "id": "79c4d2cd"
      },
      "source": [
        "<p style=\"\n",
        "  background: #ffffff;\n",
        "  border: 5px solid #9f0b0bff;\n",
        "  border-radius: 18px;\n",
        "  padding: 30px 180px;\n",
        "  text-align: center;\n",
        "  text-shadow: 1px 1px 3px rgba(0, 0, 0, 0.4);\n",
        "  font-family: 'serif';\n",
        "  color: #9f0b0bff;\n",
        "  font-size: 3.8rem;\n",
        "  width: fit-content;\n",
        "  margin: 20px auto;\n",
        "\">\n",
        "  Data Loading <br>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "fcae08a5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "fcae08a5",
        "outputId": "b50ed2f3-8eda-4f8f-9b64-3fa5e7c058a4"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'C:\\\\sharing1\\\\ML4\\\\edu\\\\nlp\\\\uci-news-aggregator.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2139138833.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'C:\\\\sharing1\\\\ML4\\\\edu\\\\nlp\\\\uci-news-aggregator.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TITLE'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'CATEGORY'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Shuffle dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mconcated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrac\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\sharing1\\\\ML4\\\\edu\\\\nlp\\\\uci-news-aggregator.csv'"
          ]
        }
      ],
      "source": [
        "data_path = 'C:\\\\sharing1\\\\ML4\\\\edu\\\\nlp\\\\uci-news-aggregator.csv'\n",
        "data = pd.read_csv(data_path, usecols=['TITLE', 'CATEGORY'])\n",
        "\n",
        "# Shuffle dataset\n",
        "concated = data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Encode labels\n",
        "label_map = {'e':0, 'b':1, 't':2, 'm':3}\n",
        "concated['LABEL'] = concated['CATEGORY'].map(label_map)\n",
        "concated.drop(['CATEGORY'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ced8e497",
      "metadata": {
        "id": "ced8e497"
      },
      "source": [
        "<p style=\"\n",
        "  background: #ffffff;\n",
        "  border: 5px solid #9f0b0bff;\n",
        "  border-radius: 18px;\n",
        "  padding: 40px 180px;\n",
        "  text-align: center;\n",
        "  text-shadow: 1px 1px 3px rgba(0, 0, 0, 0.4);\n",
        "  font-family: 'serif';\n",
        "  color: #9f0b0bff;\n",
        "  font-size: 3.8rem;\n",
        "  width: fit-content;\n",
        "  margin: 20px auto;\n",
        "\">\n",
        "  Text Preprocessing <br>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dddbadf3",
      "metadata": {
        "id": "dddbadf3"
      },
      "outputs": [],
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens = [t for t in tokens if t.isalpha() and t not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "concated['TOKENS'] = concated['TITLE'].apply(preprocess_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3e2b15f",
      "metadata": {
        "id": "a3e2b15f"
      },
      "source": [
        "<p style=\"\n",
        "  background: #ffffff;\n",
        "  border: 5px solid #9f0b0bff;\n",
        "  border-radius: 18px;\n",
        "  padding: 40px 180px;\n",
        "  text-align: center;\n",
        "  text-shadow: 1px 1px 3px rgba(0, 0, 0, 0.4);\n",
        "  font-family: 'serif';\n",
        "  color: #9f0b0bff;\n",
        "  font-size: 3.8rem;\n",
        "  width: fit-content;\n",
        "  margin: 20px auto;\n",
        "\">\n",
        "  Build Vocab <br>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2b365d2",
      "metadata": {
        "id": "f2b365d2"
      },
      "outputs": [],
      "source": [
        "all_tokens = [token for tokens in concated['TOKENS'] for token in tokens]\n",
        "\n",
        "MAX_VOCAB = 20000\n",
        "counter = Counter(all_tokens)\n",
        "most_common = counter.most_common(MAX_VOCAB)\n",
        "\n",
        "word2idx = {word: i+1 for i, (word, _) in enumerate(most_common)}\n",
        "vocab_size = len(word2idx) + 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93638be8",
      "metadata": {
        "id": "93638be8"
      },
      "source": [
        "<p style=\"\n",
        "  background: #ffffff;\n",
        "  border: 5px solid #9f0b0bff;\n",
        "  border-radius: 18px;\n",
        "  padding: 40px 180px;\n",
        "  text-align: center;\n",
        "  text-shadow: 1px 1px 3px rgba(0, 0, 0, 0.4);\n",
        "  font-family: 'serif';\n",
        "  color: #9f0b0bff;\n",
        "  font-size: 3.8rem;\n",
        "  width: fit-content;\n",
        "  margin: 20px auto;\n",
        "\">\n",
        "  Tokens To Sequences <br>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1268d2f",
      "metadata": {
        "id": "d1268d2f"
      },
      "outputs": [],
      "source": [
        "max_len = 30\n",
        "\n",
        "def tokens_to_sequence(tokens):\n",
        "    seq = [word2idx.get(word, 0) for word in tokens]\n",
        "    if len(seq) < max_len:\n",
        "        seq += [0] * (max_len - len(seq))\n",
        "    else:\n",
        "        seq = seq[:max_len]\n",
        "    return seq\n",
        "\n",
        "concated['SEQ'] = concated['TOKENS'].apply(tokens_to_sequence)\n",
        "\n",
        "X = np.array(concated['SEQ'].tolist())\n",
        "y = concated['LABEL'].values"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e9d9c18",
      "metadata": {
        "id": "3e9d9c18"
      },
      "source": [
        "<p style=\"\n",
        "  background: #ffffff;\n",
        "  border: 5px solid #9f0b0bff;\n",
        "  border-radius: 18px;\n",
        "  padding: 40px 180px;\n",
        "  text-align: center;\n",
        "  text-shadow: 1px 1px 3px rgba(0, 0, 0, 0.4);\n",
        "  font-family: 'serif';\n",
        "  color: #9f0b0bff;\n",
        "  font-size: 3.8rem;\n",
        "  width: fit-content;\n",
        "  margin: 20px auto;\n",
        "\">\n",
        "  Train-Test Split <br>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ac200d4",
      "metadata": {
        "id": "4ac200d4"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42, stratify=y\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c887ad04",
      "metadata": {
        "id": "c887ad04"
      },
      "source": [
        "<p style=\"\n",
        "  background: #ffffff;\n",
        "  border: 5px solid #9f0b0bff;\n",
        "  border-radius: 18px;\n",
        "  padding: 40px 180px;\n",
        "  text-align: center;\n",
        "  text-shadow: 1px 1px 3px rgba(0, 0, 0, 0.4);\n",
        "  font-family: 'serif';\n",
        "  color: #9f0b0bff;\n",
        "  font-size: 3.8rem;\n",
        "  width: fit-content;\n",
        "  margin: 20px auto;\n",
        "\">\n",
        "  Pytorch Dataset <br>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56f5d108",
      "metadata": {
        "id": "56f5d108"
      },
      "outputs": [],
      "source": [
        "class NewsDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.LongTensor(X)\n",
        "        self.y = torch.LongTensor(y)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "batch_size = 512\n",
        "train_loader = DataLoader(NewsDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(NewsDataset(X_test, y_test), batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6782ce38",
      "metadata": {
        "id": "6782ce38"
      },
      "source": [
        "<p style=\"\n",
        "  background: #ffffff;\n",
        "  border: 5px solid #9f0b0bff;\n",
        "  border-radius: 18px;\n",
        "  padding: 40px 180px;\n",
        "  text-align: center;\n",
        "  text-shadow: 1px 1px 3px rgba(0, 0, 0, 0.4);\n",
        "  font-family: 'serif';\n",
        "  color: #9f0b0bff;\n",
        "  font-size: 3.8rem;\n",
        "  width: fit-content;\n",
        "  margin: 20px auto;\n",
        "\">\n",
        "  LSTM Model <br>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "704ee0e7",
      "metadata": {
        "id": "704ee0e7"
      },
      "outputs": [],
      "source": [
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(\n",
        "            embedding_dim,\n",
        "            hidden_dim,\n",
        "            batch_first=True,\n",
        "            bidirectional=True\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        _, (hn, _) = self.lstm(x)\n",
        "        hn = torch.cat((hn[-2], hn[-1]), dim=1)\n",
        "        out = self.dropout(hn)\n",
        "        return self.fc(out)\n",
        "\n",
        "embedding_dim = 128\n",
        "hidden_dim = 128\n",
        "output_dim = 4\n",
        "\n",
        "device = torch.device('cpu')\n",
        "model = LSTMClassifier(vocab_size, embedding_dim, hidden_dim, output_dim).to(device)\n",
        "\n",
        "# Class weights\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a5d6ff6",
      "metadata": {
        "id": "8a5d6ff6"
      },
      "source": [
        "<p style=\"\n",
        "  background: #ffffff;\n",
        "  border: 5px solid #9f0b0bff;\n",
        "  border-radius: 18px;\n",
        "  padding: 40px 180px;\n",
        "  text-align: center;\n",
        "  text-shadow: 1px 1px 3px rgba(0, 0, 0, 0.4);\n",
        "  font-family: 'serif';\n",
        "  color: #9f0b0bff;\n",
        "  font-size: 3.8rem;\n",
        "  width: fit-content;\n",
        "  margin: 20px auto;\n",
        "\">\n",
        "  Training <br>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3f3e601",
      "metadata": {
        "id": "a3f3e601"
      },
      "outputs": [],
      "source": [
        "epochs = 5\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28df482c",
      "metadata": {
        "id": "28df482c"
      },
      "source": [
        "<p style=\"\n",
        "  background: #ffffff;\n",
        "  border: 5px solid #9f0b0bff;\n",
        "  border-radius: 18px;\n",
        "  padding: 40px 180px;\n",
        "  text-align: center;\n",
        "  text-shadow: 1px 1px 3px rgba(0, 0, 0, 0.4);\n",
        "  font-family: 'serif';\n",
        "  color: #9f0b0bff;\n",
        "  font-size: 3.8rem;\n",
        "  width: fit-content;\n",
        "  margin: 20px auto;\n",
        "\">\n",
        "  Evaluation <br>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "622e3152",
      "metadata": {
        "id": "622e3152"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "all_preds, all_labels = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        X_batch = X_batch.to(device)\n",
        "        outputs = model(X_batch)\n",
        "        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
        "\n",
        "        all_preds.extend(preds)\n",
        "        all_labels.extend(y_batch.numpy())\n",
        "\n",
        "print(\"Test Accuracy:\", accuracy_score(all_labels, all_preds))\n",
        "print(classification_report(\n",
        "    all_labels,\n",
        "    all_preds,\n",
        "    target_names=['entertainment','business','science/tech','health']\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14655a2c",
      "metadata": {
        "id": "14655a2c"
      },
      "source": [
        "<p style=\"\n",
        "  background: #ffffff;\n",
        "  border: 5px solid #9f0b0bff;\n",
        "  border-radius: 18px;\n",
        "  padding: 40px 180px;\n",
        "  text-align: center;\n",
        "  text-shadow: 1px 1px 3px rgba(0, 0, 0, 0.4);\n",
        "  font-family: 'serif';\n",
        "  color: #9f0b0bff;\n",
        "  font-size: 3.8rem;\n",
        "  width: fit-content;\n",
        "  margin: 20px auto;\n",
        "\">\n",
        "  Prediction Example <br>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a8738e7",
      "metadata": {
        "id": "1a8738e7"
      },
      "outputs": [],
      "source": [
        "txt = [\"Regular fast food eating linked to fertility issues in women\"]\n",
        "tokens = preprocess_text(txt[0])\n",
        "seq = tokens_to_sequence(tokens)\n",
        "seq_tensor = torch.LongTensor(seq).unsqueeze(0).to(device)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    pred = model(seq_tensor)\n",
        "    pred_label = pred.argmax(dim=1).item()\n",
        "\n",
        "labels = ['entertainment','business','science/tech','health']\n",
        "print(\"Prediction:\", labels[pred_label])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}